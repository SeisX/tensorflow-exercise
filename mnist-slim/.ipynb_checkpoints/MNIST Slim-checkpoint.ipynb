{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using slim to Build Model Architectures\n",
    "\n",
    "This notebook gives a simple example of using Tensorflow's [slim library](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) to construct a \"deeper\" learning architecture for the MNIST dataset. We'll see how easy it is to construct architectures, and how to output simple summaries from Tensorboard.\n",
    "\n",
    "As a simple example, we'll use the architecture presented in Michael Nielsen's online [textbook](http://neuralnetworksanddeeplearning.com/chap6.html). This knowledge will later become useful as we explore more complicated architectures like [Inception](https://github.com/tensorflow/models/blob/master/inception/inception/slim/inception_model.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Read in MNIST dataset, compute mean / standard deviation of the training images\n",
    "mnist = mnist_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "MEAN = np.mean(mnist.train.images)\n",
    "STD = np.std(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience method for reshaping images. The included MNIST dataset stores images\n",
    "# as Nx784 row vectors. This method reshapes the inputs into Nx28x28x1 images that are\n",
    "# better suited for convolution operations and rescales the inputs so they have a\n",
    "# mean of 0 and unit variance.\n",
    "def resize_images(images):\n",
    "    reshaped = (images - MEAN)/STD\n",
    "    reshaped = np.reshape(reshaped, [-1, 28, 28, 1])\n",
    "    \n",
    "    assert(reshaped.shape[1] == 28)\n",
    "    assert(reshaped.shape[2] == 28)\n",
    "    assert(reshaped.shape[3] == 1)\n",
    "    \n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NielsenNet\n",
    "\n",
    "The neural net architecture presented by Michael Nielsen in chapter 6 of his textbook achieves an accuracy in excess of 99%. I've dubbed this architecture _NielsenNet_. It consists of two convolution layers, followed by two fully connected neural network layers, followed by an output layer. Dropout is used to after each fully-connected layer to control overfitting.\n",
    "\n",
    "When building this model using slim, we'll use the built-in `conv2d` and `max_pool` functions to build the convlution layers, changing 28x28 input images into 5x5x40 outputs for the fully connected layer. We'll do this with a combination of different padding modes (`SAME` vs `VALID`) and max-pooling.\n",
    "\n",
    "Finally we'll build a succession of fully-connected layers using the `fully_connected` convenience method. Dropout can be implemented using slim's `dropout` method, gated by the `is_training` tensor.\n",
    "\n",
    "Building the network by successively mutating the `net` variable is pretty common, and something we'll see later on in the Inception architecture ([peek here](https://github.com/tensorflow/models/blob/master/inception/inception/slim/inception_model.py#L107))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nielsen_net(inputs, is_training, scope='NielsenNet'):\n",
    "    with tf.variable_scope(scope, 'NielsenNet'):\n",
    "        # First Group: Convolution + Pooling 28x28x1 => 28x28x20 => 14x14x20\n",
    "        net = slim.conv2d(inputs, 20, [5, 5], padding='SAME', scope='layer1-conv')\n",
    "        net = slim.max_pool2d(net, 2, stride=2, scope='layer2-max-pool')\n",
    "\n",
    "        # Second Group: Convolution + Pooling 14x14x20 => 10x10x40 => 5x5x40\n",
    "        net = slim.conv2d(net, 40, [5, 5], padding='VALID', scope='layer3-conv')\n",
    "        net = slim.max_pool2d(net, 2, stride=2, scope='layer4-max-pool')\n",
    "\n",
    "        # Reshape: 5x5x40 => 1000x1\n",
    "        net = tf.reshape(net, [-1, 5*5*40])\n",
    "\n",
    "        # Fully Connected Layer: 1000x1 => 1000x1\n",
    "        net = slim.fully_connected(net, 1000, scope='layer5')\n",
    "        net = slim.dropout(net, is_training=is_training, scope='layer5-dropout')\n",
    "\n",
    "        # Second Fully Connected: 1000x1 => 1000x1\n",
    "        net = slim.fully_connected(net, 1000, scope='layer6')\n",
    "        net = slim.dropout(net, is_training=is_training, scope='layer6-dropout')\n",
    "\n",
    "        # Output Layer: 1000x1 => 10x1\n",
    "        net = slim.fully_connected(net, 10, scope='output')\n",
    "        net = slim.dropout(net, is_training=is_training, scope='output-dropout')\n",
    "\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Create the placeholder tensors for the input images (x), the training labels (y_actual)\n",
    "# and whether or not dropout is active (is_training)\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='Inputs')\n",
    "y_actual = tf.placeholder(tf.float32, shape=[None, 10], name='Labels')\n",
    "is_training = tf.placeholder(tf.bool, name='IsTraining')\n",
    "\n",
    "# Pass the inputs into nielsen_net, outputting the logits\n",
    "logits = nielsen_net(x, is_training, scope='NielsenNetTrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the logits to create four additional operations:\n",
    "#\n",
    "# 1: The cross entropy of the predictions vs. the actual labels\n",
    "# 2: The number of correct predictions\n",
    "# 3: The accuracy given the number of correct predictions\n",
    "# 4: The update step, using the MomentumOptimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_actual, logits=logits))\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_actual, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "train_step = tf.train.MomentumOptimizer(0.01, 0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To monitor our progress using tensorboard, create two summary operations\n",
    "# to track the loss and the accuracy\n",
    "loss_summary = tf.summary.scalar('loss', cross_entropy)\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_writer = tf.summary.FileWriter('/tmp/nielsen-net', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0, Validation Accuracy = 97.40%\n",
      "Step:  1000, Validation Accuracy = 98.02%\n",
      "Step:  2000, Validation Accuracy = 98.24%\n",
      "Step:  3000, Validation Accuracy = 98.50%\n",
      "Step:  4000, Validation Accuracy = 98.76%\n",
      "Step:  5000, Validation Accuracy = 98.90%\n",
      "Step:  6000, Validation Accuracy = 98.98%\n",
      "Step:  7000, Validation Accuracy = 98.92%\n",
      "Step:  8000, Validation Accuracy = 99.02%\n",
      "Step:  9000, Validation Accuracy = 99.08%\n"
     ]
    }
   ],
   "source": [
    "eval_data = {\n",
    "    x: resize_images(mnist.validation.images),\n",
    "    y_actual: mnist.validation.labels,\n",
    "    is_training: False\n",
    "}\n",
    "\n",
    "# considering the efficiency, I change range(100000) to range(10000) to test this notebook\n",
    "for i in range(10000):\n",
    "    images, labels = mnist.train.next_batch(100)\n",
    "    summary, _ = sess.run([loss_summary, train_step], feed_dict={x: resize_images(images), y_actual: labels, is_training: True})\n",
    "    train_writer.add_summary(summary, i)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        summary, acc = sess.run([accuracy_summary, accuracy], feed_dict=eval_data)\n",
    "        train_writer.add_summary(summary, i)\n",
    "        print(\"Step: %5d, Validation Accuracy = %5.2f%%\" % (i, acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0, Test Accuracy = 99.24%\n",
      "Step:     1, Test Accuracy = 98.92%\n",
      "Step:     2, Test Accuracy = 99.10%\n",
      "Step:     3, Test Accuracy = 99.06%\n",
      "Step:     4, Test Accuracy = 99.14%\n"
     ]
    }
   ],
   "source": [
    "# Due to memory limitation, and on the basis of good running above,\n",
    "# sample number small than that of eval_data (that is 5500) will be ok.\n",
    "# While, the number size of test_data is 10000, so I choose 5000, \n",
    "# and do a 5-loop repetition.\n",
    "for i in range(5):\n",
    "    images, labels = mnist.test.next_batch(5000)\n",
    "    summary, acc = sess.run([accuracy_summary, accuracy], feed_dict={x: resize_images(images), y_actual: labels, is_training: False})\n",
    "    print(\"Step: %5d, Test Accuracy = %5.2f%%\" % (i, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
